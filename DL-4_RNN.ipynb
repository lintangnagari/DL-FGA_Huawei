{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.2 PROCEDURE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 10\n",
    "step = 3\n",
    "embed_size = 128\n",
    "hidden_size = 128\n",
    "batch_size = 64\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, diversity=1.0):\n",
    "   preds = np.asarray(preds).astype('float64')\n",
    "   preds = np.log(preds + 1e-10) / diversity\n",
    "   exp_preds = np.exp(preds)\n",
    "   preds = exp_preds / np.sum(exp_preds)\n",
    "   probas = np.random.multinomial(1, preds, 1)\n",
    "   return np.argmax(probas)\n",
    "def preprocess(source_file):\n",
    "   sentences = []\n",
    "   \n",
    "   with open(source_file, 'r', encoding='utf8') as fr:\n",
    "         lines = fr.readlines()\n",
    "         for line in lines:\n",
    "               line = line.strip()\n",
    "               count = 0\n",
    "               for c in line:\n",
    "                     if (c >= 'a' and c <= 'z') or (c >= 'A' and c <= 'Z'):\n",
    "                           count += 1\n",
    "               if count / len(line) < 0.1:\n",
    "                     sentences.append(line)\n",
    "                     \n",
    "   return sentences\n",
    "\n",
    "#preprocess\n",
    "sentences = preprocess('Data/data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bi_map():\n",
    "   chars = {}\n",
    "   for sentence in sentences:\n",
    "      for c in sentence:\n",
    "         chars[c] = chars.get(c, 0) + 1\n",
    "   chars = sorted(chars.items(), key=lambda x: x[1], reverse=True)\n",
    "# print('sorting result: \\n', chars[:5])\n",
    "   chars = [char[0] for char in chars]\n",
    "   vocab_size = len(chars)\n",
    "# print('sorting result: \\n', chars[:5])\n",
    "# print('total number of words:\\n', vocab_size)\n",
    "   char2id = {c: i for i, c in enumerate(chars)}\n",
    "   id2char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "   with open('./data/dictionary.pkl', 'wb') as fw:\n",
    "      pickle.dump([char2id, id2char], fw)\n",
    "   return char2id, id2char, vocab_size\n",
    "#bi_map()\n",
    "char2id, id2char, vocab_size = bi_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample\n",
    "def on_epoch_end(epoch, logs):\n",
    "   index = random.randint(0, len(sentences))\n",
    "   for diversity in [0.2, 0.5, 1.0]:\n",
    "      print('----- diversity:', diversity)\n",
    "      sentence = sentences[index][:maxlen]\n",
    "      print('----- Generating with seed: ' + sentence)\n",
    "      sys.stdout.write(sentence)\n",
    "      for i in range(400):\n",
    "         x_pred = np.zeros((1, maxlen))\n",
    "         for t, char in enumerate(sentence):\n",
    "            x_pred[0, t] = char2id[char]\n",
    "         preds = model.predict(x_pred, verbose=0)[0]\n",
    "         next_index = sample(preds, diversity)\n",
    "         next_char = id2char[next_index]\n",
    "         sentence = sentence[1:] + next_char\n",
    "\n",
    "def training_data_labels():\n",
    "   X_data = []\n",
    "   Y_data = []\n",
    "   for sentence in sentences:\n",
    "      for i in range(0, len(sentence) - maxlen, step):\n",
    "         X_data.append([char2id[c] for c in sentence[i: i + maxlen]])\n",
    "         y = np.zeros(vocab_size, dtype=np.bool)\n",
    "         y[char2id[sentence[i + maxlen]]] = 1\n",
    "         Y_data.append(y)\n",
    "   X_data = np.array(X_data)\n",
    "   Y_data = np.array(Y_data)\n",
    "   X_data = X_data[:2000]\n",
    "   Y_data = Y_data[:2000]\n",
    "   return X_data, Y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Both `input_dim` and `output_dim` should be positive, Received input_dim = 0 and output_dim = 128",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[39m=\u001b[39m Sequential()\n\u001b[1;32m----> 2\u001b[0m model\u001b[39m.\u001b[39madd(Embedding(input_dim\u001b[39m=\u001b[39;49mvocab_size, output_dim\u001b[39m=\u001b[39;49membed_size, input_length\u001b[39m=\u001b[39;49mmaxlen))\n\u001b[0;32m      3\u001b[0m model\u001b[39m.\u001b[39madd(LSTM(hidden_size, input_shape\u001b[39m=\u001b[39m(maxlen, embed_size)))\n\u001b[0;32m      4\u001b[0m model\u001b[39m.\u001b[39madd(Dense(vocab_size, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m'\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Lintang\\anaconda3\\envs\\MS20_1\\lib\\site-packages\\keras\\dtensor\\utils.py:96\u001b[0m, in \u001b[0;36mallow_initializer_layout.<locals>._wrap_function\u001b[1;34m(layer_instance, *args, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[39mif\u001b[39;00m layout:\n\u001b[0;32m     94\u001b[0m             layout_args[variable_name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_layout\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m layout\n\u001b[1;32m---> 96\u001b[0m init_method(layer_instance, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     98\u001b[0m \u001b[39m# Inject the layout parameter after the invocation of __init__()\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[39mfor\u001b[39;00m layout_param_name, layout \u001b[39min\u001b[39;00m layout_args\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[1;32mc:\\Users\\Lintang\\anaconda3\\envs\\MS20_1\\lib\\site-packages\\keras\\layers\\core\\embedding.py:132\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[1;34m(self, input_dim, output_dim, embeddings_initializer, embeddings_regularizer, activity_regularizer, embeddings_constraint, mask_zero, input_length, **kwargs)\u001b[0m\n\u001b[0;32m    130\u001b[0m         kwargs[\u001b[39m\"\u001b[39m\u001b[39minput_shape\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m (\u001b[39mNone\u001b[39;00m,)\n\u001b[0;32m    131\u001b[0m \u001b[39mif\u001b[39;00m input_dim \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m output_dim \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 132\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    133\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mBoth `input_dim` and `output_dim` should be positive, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    134\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReceived input_dim = \u001b[39m\u001b[39m{\u001b[39;00minput_dim\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    135\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mand output_dim = \u001b[39m\u001b[39m{\u001b[39;00moutput_dim\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    136\u001b[0m     )\n\u001b[0;32m    137\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    138\u001b[0m     \u001b[39mnot\u001b[39;00m base_layer_utils\u001b[39m.\u001b[39mv2_dtype_behavior_enabled()\n\u001b[0;32m    139\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m kwargs\n\u001b[0;32m    140\u001b[0m ):\n\u001b[0;32m    141\u001b[0m     \u001b[39m# In TF1, the dtype defaults to the input dtype which is typically\u001b[39;00m\n\u001b[0;32m    142\u001b[0m     \u001b[39m# int32, so explicitly set it to floatx\u001b[39;00m\n\u001b[0;32m    143\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m backend\u001b[39m.\u001b[39mfloatx()\n",
      "\u001b[1;31mValueError\u001b[0m: Both `input_dim` and `output_dim` should be positive, Received input_dim = 0 and output_dim = 128"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=maxlen))\n",
    "model.add(LSTM(hidden_size, input_shape=(maxlen, embed_size)))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary()\n",
    "\n",
    "X_data, Y_data = training_data_labels()\n",
    "\n",
    "#on_epoch_end\n",
    "model.fit(X_data, Y_data, batch_size=batch_size, epochs=epochs,\n",
    "callbacks=[LambdaCallback(on_epoch_end=on_epoch_end)])\n",
    "\n",
    "model.save('./model/song_tensorflow.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file or directory found at ./model/song_tensorflow.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[0;32m      5\u001b[0m maxlen \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[1;32m----> 6\u001b[0m model \u001b[39m=\u001b[39m load_model(\u001b[39m'\u001b[39;49m\u001b[39m./model/song_tensorflow.h5\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      8\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m./data/dictionary.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m fr:\n\u001b[0;32m      9\u001b[0m    [char2id, id2char] \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(fr)\n",
      "File \u001b[1;32mc:\\Users\\Lintang\\anaconda3\\envs\\MS20_1\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Lintang\\anaconda3\\envs\\MS20_1\\lib\\site-packages\\keras\\saving\\save.py:226\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(filepath_str, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    225\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39mexists(filepath_str):\n\u001b[1;32m--> 226\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(\n\u001b[0;32m    227\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo file or directory found at \u001b[39m\u001b[39m{\u001b[39;00mfilepath_str\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    228\u001b[0m         )\n\u001b[0;32m    230\u001b[0m     \u001b[39mif\u001b[39;00m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39misdir(filepath_str):\n\u001b[0;32m    231\u001b[0m         \u001b[39mreturn\u001b[39;00m saved_model_load\u001b[39m.\u001b[39mload(\n\u001b[0;32m    232\u001b[0m             filepath_str, \u001b[39mcompile\u001b[39m, options\n\u001b[0;32m    233\u001b[0m         )\n",
      "\u001b[1;31mOSError\u001b[0m: No file or directory found at ./model/song_tensorflow.h5"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "maxlen = 10\n",
    "model = load_model('./model/song_tensorflow.h5')\n",
    "\n",
    "with open('./data/dictionary.pkl', 'rb') as fr:\n",
    "   [char2id, id2char] = pickle.load(fr)\n",
    "\n",
    "def sample(preds, diversity=1.0):\n",
    "   preds = np.asarray(preds).astype('float64')\n",
    "   preds = np.log(preds + 1e-10) / diversity\n",
    "   exp_preds = np.exp(preds)\n",
    "   preds = exp_preds / np.sum(exp_preds)\n",
    "   probas = np.random.multinomial(1, preds, 1)\n",
    "   return np.argmax(probas)\n",
    "\n",
    "sentence = \"Enter new lyrics.\"\n",
    "\n",
    "sentence = sentence[:maxlen]\n",
    "\n",
    "diversity = 1.0\n",
    "print('----- Generating with seed: ' + sentence)\n",
    "print('----- diversity:', diversity)\n",
    "\n",
    "sys.stdout.write(sentence)\n",
    "\n",
    "for i in range(40):\n",
    "   x_pred = np.zeros((1, maxlen))\n",
    "   for t, char in enumerate(sentence):\n",
    "      x_pred[0, t] = char2id[char]\n",
    "   preds = model.predict(x_pred, verbose=0)[0]\n",
    "   next_index = sample(preds, diversity)\n",
    "   next_char = id2char[next_index]\n",
    "   sentence = sentence[1:] + next_char\n",
    "   sys.stdout.write(next_char)\n",
    "   sys.stdout.flush()\n",
    "\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MS20_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
